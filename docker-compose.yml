# References:
# bitnami/spark: https://raw.githubusercontent.com/bitnami/containers/main/bitnami/spark/docker-compose.yml
# apache/hadoop: https://hub.docker.com/r/apache/hadoop
# apache/hive: https://hive.apache.org/developement/quickstart/

x-hdfs-common:
  &hdfs-common
  image: apache/hadoop:3
  env_file:
    - ./containers/hadoop-hive/hadoop-hive.env


x-hive-common:
  &hive-common
  image: apache/hive:4.0.0-beta-1
  env_file:
    - ./containers/hadoop-hive/hadoop-hive.env
  environment:
    - HIVE_CUSTOM_CONF_DIR=/opt/hive/conf/custom
  volumes:
    - ./containers/hadoop-hive/hive-site.xml:/opt/hive/conf/custom/hive-site.xml


services:
  airflow:
    build: 
      context: ./containers/airflow
    env_file: ./containers/airflow/.env
    volumes:
      - ./containers/airflow/dags:/opt/airflow/dags
      - ./containers/airflow/config:/opt/airflow/config
      - ./logs/airflow:/opt/airflow/logs
      - ./data:/data
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    ports:
      - "8080:8080"

  spark-master:
    image: bitnami/spark:3.5
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - '8088:8080'
    volumes:
      - ./logs/spark:/opt/bitnami/spark/logs
      - ./data:/data

  spark-worker:
    image: bitnami/spark:3.5
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark

  hdfs-namenode:
    <<: *hdfs-common
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    environment:
      - ENSURE_NAMENODE_DIR=/tmp/hadoop-hadoop/dfs/name
    healthcheck:
      test: ["CMD", "hdfs", "fsck", "/"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 10s
  
  hdfs-datanode:
    <<: *hdfs-common
    command: ["hdfs", "datanode"]

  hive-server:
    <<: *hive-common
    environment:
      - SERVICE_NAME=hiveserver2
    depends_on:
      - hdfs-namenode
      - hdfs-datanode
    ports:
      - 10000:10000
      - 10002:10002
  
  hive-metastore:
    <<: *hive-common
    environment:
      - SERVICE_NAME=metastore
    ports:
      - 9083:9083
      